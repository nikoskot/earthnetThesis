# Data
batchSize: 8
numWorkers: 8    # Number of workers for Dataloaders
trainDataDir: "/home/nikoskot/earthnetThesis/EarthnetDataset/train"
testDataDir: "/home/nikoskot/earthnetThesis/EarthnetDataset/"
dataDtype: "float32"
trainDataSubset: null
trainSplit: 0.8
validationSplit: 0.2
cropMesodynamic: True
experimentsFolder: "/home/nikoskot/earthnetThesis/experiments/videoSwinUnetV8/"
overfitTraining: False
visualizationFreq: 10
calculateENSonValidationFreq: 10

# Model
modelType: "videoSwinUnetV8"
pretrained: null #"/home/nikoskot/earthnetThesis/swin_tiny_patch4_window7_224_22k.pth"
pretrained2D: True
C: 96            # C of Video Swin Transformer (number of channels after patch embedding)
modelInputCh: 9 # Number of channels of the input data
extraInputCh: 5
staticInputCh: 1
mainInputTime: 10
outputTime: 20
modelOutputCh: 4
timeUpsampling: 'encoder' # 'patchEmbedding': do it after patch embedding, 'encoder': do it after encoder
#numBlocks: 3
inputHeightWidth: 128
windowSize: [2, 7, 7]
patchSize: [1, 2, 2]
ape: True
patchEmbedding:
  norm: "LayerNorm"       # 'LayerNorm' or 'BatchNorm' or null 
encoder:
  numChannels: [96, 192]
  layerDepths: [2, 2]
  layerNumHeads: [3, 6, 12]
  mlpRatio: 4.
  qkvBias: True
  qkScale: null
  drop: 0.
  attnDrop: 0.
  dropPath: 0.2
  norm: "LayerNorm"        # 'LayerNorm' or 'BatchNorm' or 'LOAN' or null 
  loanType: "LayerNorm"    # 'LayerNorm' or 'BatchNorm' or null    # This is relevant only if norm = LOAN
  loanNormalizeInput: False     # This is relevant only if norm = LOAN
  downsample: [False, True]
  downsampleNorm: "LOAN"   # 'LayerNorm' or 'BatchNorm' or 'LOAN' or null 
  downsampleLoanType: "LayerNorm"    # 'LayerNorm' or 'BatchNorm' or null   # This is relevant only if norm = LOAN
  downsampleLoanNormalizeInput: False      # This is relevant only if norm = LOAN
  useCheckpoint: False
  # patchMerging:
  #   norm: False
# bottleneck:
#   depth: 2
#   numHeads: 12
#   norm: False
decoder:
  numChannels: [192, 192]
  layerDepths: [2, 2]
  layerNumHeads: [3, 6, 12]
  mlpRatio: 4.
  qkvBias: True
  qkScale: null
  drop: 0.
  attnDrop: 0.
  dropPath: 0.2
  norm: "LayerNorm"        # 'LayerNorm' or 'BatchNorm' or 'LOAN' or null 
  loanType: "LayerNorm"    # 'LayerNorm' or 'BatchNorm' or null      # This is relevant only if norm = LOAN
  loanNormalizeInput: False         # This is relevant only if norm = LOAN
  upsample: [True, False]
  upsampleNorm: "LOAN"   # 'LayerNorm' or 'BatchNorm' or 'LOAN' or null 
  upsampleLoanType: "LayerNorm"    # 'LayerNorm' or 'BatchNorm' or null      # This is relevant only if norm = LOAN
  upsampleLoanNormalizeInput: False         # This is relevant only if norm = LOAN
  useCheckpoint: False
  # patchExpansion:
  #   norm: False


# Optimization
epochs: 90      # Number of epochs to run the training for
l1Weight: 1
mseWeight: 1
ssimWeight: 1
vggWeight: 1
trainingOptimizer: "adamw"
lr: 0.0002         # Def 0.0005
weightDecay: 0.05
scheduler: "CosineAnnealing"
warmupEpochs: 5
lrScaleFactor: 0.2
schedulerPatience: 5
cosineAnnealingRestartsInterval: 10
gradientClipping: False
gradientClipValue: 0.5

useMSE: False
useSSIM: True
useVGG: False
