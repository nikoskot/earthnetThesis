# Data
batchSize: 1
numWorkers: 8    # Number of workers for Dataloaders
trainDataDir: "/home/nikoskot/earthnetThesis/EarthnetDataset/train"
testDataDir: "/home/nikoskot/earthnetThesis/EarthnetDataset/"
dataDtype: "float32"
trainDataSubset: 1
trainSplit: 0.8
validationSplit: 0.2
cropMesodynamic: True
experimentsFolder: "/home/nikoskot/earthnetThesis/experiments"
overfitTraining: True


# Model
modelType: "videoSwinUnetV1"
pretrained: null #"/home/nikoskot/earthnetThesis/swin_tiny_patch4_window7_224_22k.pth"
pretrained2D: True
C: 96            # C of Video Swin Transformer (number of channels after patch embedding)
modelInputCh: 11 # Number of channels of the input data
extraInputCh: 5
mainInputTime: 10
outputTime: 20
modelOutputCh: 4
timeUpsampling: 'encoder' # 'patchEmbedding': do it after patch embedding, 'encoder': do it after encoder
#numBlocks: 3
inputHeightWidth: 128
windowSize: [3 ,7, 7]
patchSize: [1, 1, 1]
patchEmbedding:
  norm: False
encoder:
  numChannels: [96, 192, 384]
  layerDepths: [2, 2, 2]
  layerNumHeads: [3, 6, 12]
  mlpRatio: 4.
  qkvBias: False
  qkScale: null
  drop: 0.
  attnDrop: 0.
  dropPath: 0.
  norm: False
  downsample: null
  useCheckpoint: False
  patchMerging:
    norm: False
bottleneck:
  depth: 2
  numHeads: 12
  norm: False
decoder:
  numChannels: [384, 384, 288]
  layerDepths: [2, 2, 2]
  layerNumHeads: [3, 6, 12]
  mlpRatio: 4.
  qkvBias: False
  qkScale: null
  drop: 0.
  attnDrop: 0.
  dropPath: 0.
  norm: False
  downsample: null
  useCheckpoint: False
  patchExpansion:
    norm: False


# Optimization
epochs: 500      # Number of epochs to run the training for
trainLossFunctions: ["maskedL1"] # First one is the one that is used for checkpoint saving
trainingOptimizer: "adam"
lr: 0.0007         # Def 0.0005
lrScaleFactor: 0.5
weightDecay: 0
scheduler: null
schedulerPatience: 10
gradientClipping: False
gradientClipValue: 0.5
